{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kornia\n",
    "# ! pip install kornia[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 20:22:59.752038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-03 20:22:59.752230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-03 20:22:59.805653: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-03 20:23:01.692605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Importing stock ml libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from kornia.losses import BinaryFocalLossWithLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running this notebook from 'home/dev/enefit/notebook'. \n",
    "os.chdir('..') # else adjust to point to the root of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro\n",
    "\n",
    "if 'output' in locals() and 'error' in output.stderr:\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'cuda'\u001b[0m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = None\n",
    "MODEL_DIRECTORY = \"model_save\"\n",
    "\n",
    "\n",
    "# Sections of configBertTokenizer\n",
    "# Defining some key variables that will be used later on in the training\n",
    "BALANCED = True\n",
    "# LAYERS_TO_UNFREEZE = None\n",
    "LAYERS_TO_UNFREEZE = [8, 9, 10, 11]\n",
    "\n",
    "MAX_LEN = 512\n",
    "# MAX_LEN = 1024\n",
    "\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = 32 # 32 Effective size for NASA\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = TRAIN_EFFECTIVE_BATCH_SIZE / TRAIN_BATCH_SIZE\n",
    "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "EPOCHS = 5 # 5 Epochs for NASA\n",
    "LEARNING_RATE = 1e-05 * 2 # 0.00002 Rate for NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text.iloc[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets.iloc[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=15):\n",
    "        super(SequenceClassificationModel, self).__init__()\n",
    "        self.original_name = model_name\n",
    "        self.model_name = model_name.replace(\"/\", \"_\")\n",
    "        self.l1 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        return output.logits\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        return AutoTokenizer.from_pretrained(self.original_name)\n",
    "    \n",
    "    def _set_layer_trainable(self, layer, trainable):\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def _find_and_set_encoder_layers(self, module, layer_nums, trainable):\n",
    "        if hasattr(module, 'encoder'):\n",
    "            for layer_num in layer_nums:\n",
    "                try:\n",
    "                    self._set_layer_trainable(module.encoder.layer[layer_num], trainable)\n",
    "                except IndexError:\n",
    "                    print(f\"Layer {layer_num} not found in the encoder.\")\n",
    "            return True\n",
    "        else:\n",
    "            for child in module.children():\n",
    "                if self._find_and_set_encoder_layers(child, layer_nums, trainable):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def set_trainable_layers(self, layer_nums=None):\n",
    "        if layer_nums is not None:\n",
    "            self.model_name = f'{self.model_name}_Unfrozen{layer_nums}'\n",
    "            \n",
    "        # Freeze all parameters first\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze classifier layers\n",
    "        if hasattr(self.l1, 'classifier'):\n",
    "            self._set_layer_trainable(self.l1.classifier, True)\n",
    "\n",
    "        # Attempt to find and unfreeze encoder layers\n",
    "        if not self._find_and_set_encoder_layers(self.l1, layer_nums or [], True):\n",
    "            print(\"Encoder layers not found in the model.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassificationModel(\n",
       "  (l1): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(test_df.labels[0])\n",
    "\n",
    "model = SequenceClassificationModel('bert-base-uncased', num_labels=num_labels)\n",
    "# model = SequenceClassificationModel('NASA-AIML/MIKA_SafeAeroBERT', num_labels=num_labels)\n",
    "# model = SequenceClassificationModel('allenai/longformer-base-4096', num_labels=num_labels)\n",
    "\n",
    "model.set_trainable_layers(LAYERS_TO_UNFREEZE)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (96986, 2)\n",
      "TEST Dataset: (10805, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "tokenizer = model.tokenizer()\n",
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_per_label(y_true, y_pred):\n",
    "    correct = y_pred == y_true\n",
    "    accuracy_per_label = correct.float().mean(axis=0)\n",
    "    return accuracy_per_label\n",
    "\n",
    "def binary_accuracy_averaged(y_true, y_pred):\n",
    "    accuracy_per_label = binary_accuracy_per_label(y_true, y_pred)\n",
    "    accuracy_averaged = accuracy_per_label.mean()\n",
    "    return accuracy_averaged\n",
    "\n",
    "def custom_classification_report(y_true, y_pred):\n",
    "    report = metrics.classification_report(y_true, y_pred, output_dict=True, target_names=ANOMALY_LABELS, zero_division=0)\n",
    "    accuracy = binary_accuracy_per_label(y_true, y_pred)\n",
    "    extended_accuracy_new = np.append(accuracy, [accuracy.mean()] * (len(report) - len(accuracy)))\n",
    "\n",
    "    updated_report = {}\n",
    "    for i, class_label in enumerate(report.keys()):\n",
    "        # Create a new dictionary for the class with binary accuracy\n",
    "        class_dict = {'binary_accuracy': extended_accuracy_new[i]}\n",
    "        \n",
    "        # Merge this dictionary with the existing metrics for the class\n",
    "        class_dict.update(report[class_label])\n",
    "\n",
    "        # Update the main report dictionary\n",
    "        updated_report[class_label] = class_dict\n",
    "\n",
    "    return updated_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pos_weight = None\n",
    "if BALANCED:\n",
    "    # Compute weights for loss function\n",
    "    num_labels = len(training_set[0]['targets'])\n",
    "    pos_num = torch.zeros(num_labels).to(device)\n",
    "    for _, data in enumerate(training_loader, 0):\n",
    "        targets = data['targets'].to(device)\n",
    "        pos_num += torch.sum(targets, axis=0)\n",
    "    nobs = len(training_loader.dataset)\n",
    "    pos_weight = (nobs - pos_num) / pos_num\n",
    "\n",
    "    model.model_name += \"_BCE-Balanced\"\n",
    "\n",
    "loss_fn = BinaryFocalLossWithLogits(pos_weight=pos_weight, gamma=0.5, alpha=1)  # compute weighted loss for unbalanced dataset\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "metrics_dict = {\"Custom Classifcation Report\": lambda y_true, y_pred: custom_classification_report(y_true, y_pred)\n",
    "    # \"Binary Accuracy Macro\": lambda outputs, targets: binary_accuracy_averaged(targets, outputs, threshold=0.5),\n",
    "    # \"Binary Accuracy per Class\": binary_accuracy_per_label,\n",
    "    # \"F1 Score Micro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "    # \"F1 Score Macro\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "    # \"F1 Scores per Class\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average=None, zero_division=1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Saves the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to save.\n",
    "    epoch (int): The current epoch number.\n",
    "    file_path (str): Base directory to save the models.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f'Model saved at {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, directory='model_save', model_name=None, epoch=None):\n",
    "    \"\"\"\n",
    "    Loads the model state.\n",
    "\n",
    "    Args:\n",
    "    model (torch.nn.Module): The model to load state into.\n",
    "    file_path (str): Path to the saved model file.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    if epoch is None:\n",
    "        epoch = find_last_saved_epoch(directory, model_name)\n",
    "        if epoch == -1:\n",
    "            print(\"No saved model found.\")\n",
    "            return\n",
    "    \n",
    "    file_path = os.path.join(directory, f\"{model_name}_epoch_{epoch}.pth\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"No model file found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.to(device)\n",
    "    print(f'Model loaded from {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_saved_epoch(directory='model_save', model_name=None):\n",
    "    \"\"\"\n",
    "    Finds the last saved epoch number in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The directory where models are saved.\n",
    "\n",
    "    Returns:\n",
    "    int: The last saved epoch number. Returns -1 if no saved model is found.\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(directory):\n",
    "        return -1\n",
    "\n",
    "    saved_epochs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if model_name is None or filename.startswith(model_name):\n",
    "            parts = filename.replace('.pth', '').split('_')\n",
    "            if parts[-2] == 'epoch':\n",
    "                try:\n",
    "                    saved_epochs.append(int(parts[-1]))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    return max(saved_epochs, default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, batch_data, device, loss_fn, mode, optimizer=None, accumulate_gradients=False):\n",
    "    ids = batch_data['ids'].to(device, dtype=torch.long)\n",
    "    mask = batch_data['mask'].to(device, dtype=torch.long)\n",
    "    token_type_ids = batch_data['token_type_ids'].to(device, dtype=torch.long)\n",
    "    targets = batch_data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "    if mode == 'train':\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        if not accumulate_gradients:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "    return outputs, targets, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(metrics_dict, targets, outputs, is_logit=True, thresholds=0.5, percentile=None):\n",
    "    results = {}\n",
    "    labels = ANOMALY_LABELS\n",
    "    if is_logit:\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "    if thresholds is None:\n",
    "        thresholds = 0.5\n",
    "    # Calculate percentile is specified\n",
    "    if percentile is not None:\n",
    "        thresholds = []\n",
    "        for i in range(outputs.shape[1]):  # Iterate over each label\n",
    "            label_scores = outputs[:, i].detach().cpu().numpy()\n",
    "            threshold = np.percentile(label_scores, percentile)\n",
    "            thresholds.append(threshold)\n",
    "        thresholds = np.array(thresholds)\n",
    "\n",
    "    # Apply thresholds to outputs\n",
    "    outputs = (outputs >= torch.tensor(thresholds, device=outputs.device)).float()\n",
    "\n",
    "    for metric_name, metric_fn in metrics_dict.items():\n",
    "        if metric_name in [\"F1 Scores per Class\", \"Binary Accuracy per Class\"]:\n",
    "            metric_scores = metric_fn(targets.cpu(), outputs.cpu())  # Assuming targets and outputs are tensors\n",
    "            for i, score in enumerate(metric_scores):\n",
    "                label = labels[i] if i < len(labels) else f\"Class {i}\"\n",
    "                results[f\"{metric_name} - {label}\"] = score\n",
    "        else:\n",
    "            results[metric_name] = metric_fn(targets.cpu(), outputs.cpu())\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(val):\n",
    "    \"\"\"Helper function to format the value for printing.\"\"\"\n",
    "    if isinstance(val, (float, np.float16, np.float32, np.float64)):\n",
    "        return f\"{val:.4f}\"\n",
    "    elif isinstance(val, torch.Tensor) and val.dtype in [torch.float16, torch.float32, torch.float64]:\n",
    "        return f\"{val.item():.4f}\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def print_metrics_results(metrics_results):\n",
    "    # First, print scalar values and simple dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and not any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print simple dictionaries on a single line\n",
    "            dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in value.items()])\n",
    "            print(f\"{metric}: {dict_values}\")\n",
    "        elif not isinstance(value, dict):\n",
    "            # Print scalar values\n",
    "            print(f\"{metric}: {format_value(value)}\")\n",
    "\n",
    "    # Then, print nested dictionaries\n",
    "    for metric, value in metrics_results.items():\n",
    "        if isinstance(value, dict) and any(isinstance(v, dict) for v in value.values()):\n",
    "            # Print nested dictionaries\n",
    "            print(f\"\\n{metric}:\")\n",
    "            # Find the longest key length for formatting\n",
    "            max_key_length = max(len(str(k)) for k in value.keys())\n",
    "            for sub_key, sub_dict in value.items():\n",
    "                formatted_key = f\"{sub_key}:\".ljust(max_key_length + 2)\n",
    "                dict_values = \", \".join([f\"{k}: {format_value(v)}\" for k, v in sub_dict.items()])\n",
    "                print(f\"  {formatted_key} {dict_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_results(mode, epoch, batch, dataset_size, loss, start_time, batch_start_time, batch_size):\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    batch_time_ms = (current_time - batch_start_time) * 1000\n",
    "\n",
    "    current = (batch + 1) * batch_size\n",
    "    epoch_str = f\"Epoch: {epoch+1}, \" if epoch is not None else \"\"\n",
    "    \n",
    "    print(f\"\\r{mode.capitalize()} - {epoch_str}Batch: {batch+1} [{current:>5d}/{dataset_size:>5d}], \"\n",
    "          f\"Time: {elapsed_time:.0f}s {batch_time_ms:.0f}ms/step, Loss: {loss:>7f}\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(mode, model, loader, device, loss_fn, optimizer=None, epoch=None, accumulation_steps=None):\n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, data in enumerate(loader, 0):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        logits, targets, loss = process_batch(model, data, device, loss_fn, mode, optimizer)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if mode == 'train':\n",
    "            if accumulation_steps is not None and (batch + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Detach from the (gradient) computation graph to save on memory\n",
    "        all_outputs.append(logits.detach())\n",
    "        all_targets.append(targets.detach())\n",
    "\n",
    "        batch_size = targets.shape[0]\n",
    "        print_batch_results(mode, epoch, batch, len(loader.dataset), loss.item(), start_time, batch_start_time, batch_size)\n",
    "\n",
    "    if mode == 'train' and optimizer is not None and accumulation_steps is not None:\n",
    "        # Ensure any remaining gradients are applied\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    print()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, all_outputs, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, loss_fn, metrics_dict, device, hyperparameters=None):\n",
    "    model.eval()\n",
    "    avg_val_loss, val_outputs, val_targets = process_batches('evaluate', model, validation_loader, device, loss_fn)\n",
    "\n",
    "    # Set default values\n",
    "    thresholds = None\n",
    "    percentile = None\n",
    "\n",
    "    # Update values based on hyperparameters if provided\n",
    "    if hyperparameters:\n",
    "        thresholds = hyperparameters.get(\"thresholds\", thresholds)\n",
    "        percentile = hyperparameters.get(\"percentile\", percentile)\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, val_targets, val_outputs, thresholds=thresholds, percentile=percentile)\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Average Loss: {avg_val_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    return avg_val_loss, metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, training_loader, validation_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=1):\n",
    "    print(f\"Training Epoch {epoch + 1}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    if optimizer is not None:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss, train_outputs, train_targets = process_batches('train', model, training_loader, device, loss_fn, optimizer, epoch, accumulation_steps)\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, train_targets, train_outputs)\n",
    "\n",
    "    print(f\"Train Results:\")\n",
    "    print(f\"Average Training Loss for Epoch {epoch + 1}: {avg_train_loss:.4f}\")\n",
    "    print_metrics_results(metrics_results)\n",
    "\n",
    "    # Validation phase\n",
    "    if validation_loader is not None:\n",
    "        avg_val_loss, val_metrics_results = evaluate(model, validation_loader, loss_fn, metrics_dict, device)\n",
    "    else:\n",
    "        avg_val_loss = None\n",
    "        val_metrics_results = {}\n",
    "\n",
    "    return avg_train_loss, avg_val_loss, val_metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased_Unfrozen[8, 9, 10, 11]_BCE-Balanced'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found.\n"
     ]
    }
   ],
   "source": [
    "last_saved_epoch = find_last_saved_epoch(directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "\n",
    "start_epoch = last_saved_epoch + 1 if last_saved_epoch != -1 else 0\n",
    "if last_saved_epoch != -1:\n",
    "    load_model(model, directory=MODEL_DIRECTORY, model_name=MODEL_NAME, epoch=last_saved_epoch)\n",
    "    print(f\"Loaded model training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"No saved model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 1\n",
      "Training Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Epoch: 1, Batch: 558 [ 4464/96986], Time: 206s 373ms/step, Loss: 1.008596"
     ]
    }
   ],
   "source": [
    "if start_epoch < EPOCHS:\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    train_loss, val_loss, val_metrics = train(model, epoch, training_loader, testing_loader, optimizer, loss_fn, metrics_dict, device, accumulation_steps=8)\n",
    "    save_model(model, epoch, directory=MODEL_DIRECTORY, model_name=MODEL_NAME)\n",
    "    # Additional epoch-level processing if needed\n",
    "\n",
    "# Testing phase\n",
    "avg_test_loss, test_metrics_results = evaluate(model, testing_loader, loss_fn, metrics_dict, device)\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Average Loss: {avg_test_loss:.4f}\")\n",
    "print_metrics_results(test_metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_thresholds(logits, targets, metrics_dict, num_labels):\n",
    "    best_global_metric = -np.inf\n",
    "    best_thresholds = [0.5] * num_labels\n",
    "\n",
    "    # Iterate over a range of thresholds for each label\n",
    "    for label in range(num_labels):\n",
    "        for threshold in np.linspace(0, 1, 110):  # Example range and step size\n",
    "            temp_thresholds = best_thresholds.copy()\n",
    "            temp_thresholds[label] = threshold\n",
    "            metrics_results = calculate_metrics(metrics_dict, targets, logits, thresholds=temp_thresholds)\n",
    "            current_metric = metrics_results[\"Optimization Metric\"]\n",
    "\n",
    "            if current_metric > best_global_metric:\n",
    "                best_global_metric = current_metric\n",
    "                best_thresholds = temp_thresholds\n",
    "\n",
    "    metrics_results = calculate_metrics(metrics_dict, targets, logits, thresholds=best_thresholds)\n",
    "    return best_thresholds, metrics_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model to get logits\n",
    "_, logits, targets = process_batches('evaluate', model, testing_loader, device, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Thresholds: [0.43119266055045874, 0.5229357798165137, 0.25688073394495414, 0.29357798165137616, 0.28440366972477066, 0.25688073394495414, 0.21100917431192662, 0.22935779816513763, 0.26605504587155965, 0.14678899082568808, 0.14678899082568808, 0.21100917431192662, 0.2018348623853211, 0.03669724770642202]\n",
      "Optimization Metric: 0.5560\n",
      "\n",
      "Custom Classifcation Report:\n",
      "  Deviation / Discrepancy - Procedural:  binary_accuracy: 0.7349, precision: 0.7272, recall: 0.8778, f1-score: 0.7954, support: 6343.0000\n",
      "  Aircraft Equipment:                    binary_accuracy: 0.8610, precision: 0.8278, recall: 0.8267, f1-score: 0.8273, support: 4351.0000\n",
      "  Conflict:                              binary_accuracy: 0.8965, precision: 0.6642, recall: 0.8191, f1-score: 0.7336, support: 1879.0000\n",
      "  Inflight Event / Encounter:            binary_accuracy: 0.8155, precision: 0.5795, recall: 0.6455, f1-score: 0.6107, support: 2423.0000\n",
      "  ATC Issue:                             binary_accuracy: 0.8622, precision: 0.6748, recall: 0.7906, f1-score: 0.7281, support: 2522.0000\n",
      "  Deviation - Altitude:                  binary_accuracy: 0.8931, precision: 0.4280, recall: 0.5967, f1-score: 0.4985, support: 962.0000\n",
      "  Deviation - Track / Heading:           binary_accuracy: 0.8899, precision: 0.3760, recall: 0.5481, f1-score: 0.4460, support: 874.0000\n",
      "  Ground Event / Encounter:              binary_accuracy: 0.9196, precision: 0.5520, recall: 0.5920, f1-score: 0.5713, support: 978.0000\n",
      "  Flight Deck / Cabin / Aircraft Event:  binary_accuracy: 0.9403, precision: 0.6531, recall: 0.6178, f1-score: 0.6350, support: 908.0000\n",
      "  Ground Incursion:                      binary_accuracy: 0.9534, precision: 0.4585, recall: 0.5969, f1-score: 0.5187, support: 454.0000\n",
      "  Airspace Violation:                    binary_accuracy: 0.9372, precision: 0.3030, recall: 0.5172, f1-score: 0.3822, support: 406.0000\n",
      "  Deviation - Speed:                     binary_accuracy: 0.9583, precision: 0.3931, recall: 0.2931, f1-score: 0.3358, support: 389.0000\n",
      "  Ground Excursion:                      binary_accuracy: 0.9723, precision: 0.4914, recall: 0.5870, f1-score: 0.5350, support: 293.0000\n",
      "  No Specific Anomaly Occurred:          binary_accuracy: 0.9786, precision: 0.1223, recall: 0.2584, f1-score: 0.1661, support: 89.0000\n",
      "  micro avg:                             binary_accuracy: 0.9009, precision: 0.6481, recall: 0.7540, f1-score: 0.6971, support: 22871.0000\n",
      "  macro avg:                             binary_accuracy: 0.9009, precision: 0.5179, recall: 0.6119, f1-score: 0.5560, support: 22871.0000\n",
      "  weighted avg:                          binary_accuracy: 0.9009, precision: 0.6594, recall: 0.7540, f1-score: 0.7012, support: 22871.0000\n",
      "  samples avg:                           binary_accuracy: 0.9009, precision: 0.6949, recall: 0.7908, f1-score: 0.6989, support: 22871.0000\n"
     ]
    }
   ],
   "source": [
    "opt_metrics_dict = {\n",
    "    \"Optimization Metric\": lambda y_true, y_pred: metrics.f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "}\n",
    "opt_metrics_dict.update(metrics_dict)\n",
    "\n",
    "# Optimize thresholds\n",
    "best_thresholds, metrics_results = optimize_thresholds(logits, targets, opt_metrics_dict, num_labels=len(ANOMALY_LABELS))\n",
    "\n",
    "print(\"Optimized Thresholds:\", best_thresholds)\n",
    "print_metrics_results(metrics_results)\n",
    "\n",
    "# # Use these thresholds in your evaluation\n",
    "# avg_test_loss, test_metrics_results = evaluate(model, testing_loader, loss_fn, metrics_dict, device, hyperparameters=best_thresholds)\n",
    "# print(\"Test Results with Optimized Thresholds:\")\n",
    "# print_metrics_results(test_metrics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
