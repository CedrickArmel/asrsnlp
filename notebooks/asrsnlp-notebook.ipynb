{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24423152",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e5468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import logging\n",
    "import typing\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AutoTokenizer, AutoModelForMaskedLM\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb357b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running this notebook from 'home/dev/enefit/notebook'. \n",
    "os.chdir('..') # else adjust to point to the root of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd34bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro\n",
    "\n",
    "if 'output' in locals() and 'error' in output.stderr:\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f17cf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "train = catalog.load('train_data_final')\n",
    "test = catalog.load('test_data_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71200605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless(**kwargs) -> None:\n",
    "    \"\"\"Keep only useful columns in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (str): Name of the dataset as defined in the catalog\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    for name, item in kwargs.items():\n",
    "        filename = str(name + \"_prim.parquet\")\n",
    "        data = item[['Narrative', 'Anomaly', 'Synopsis']]\n",
    "        try:\n",
    "            logger.info(\"Storing the new dataset from %s in parquet format.\", filename)\n",
    "            data.to_parquet(os.path.join(\"data/03_primary\", filename))\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"Error occured : %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f161188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropna_row(df:pd.DataFrame, sub:list) -> pd.DataFrame:\n",
    "    \"\"\" Drop the row if there any missing value in either column in sub\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe\n",
    "        sub (list): List of columns where to look for missings\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    data = df.dropna(axis = 0, subset=sub)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b436107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cell(cell:pd.Series, labels:list) -> pd.Series:\n",
    "    \"\"\"Encode the multilabels cell such that the cell content is replaced by \\n\n",
    "    a list of same length as labels and containing 0/1.\n",
    "\n",
    "    Args:\n",
    "        cell (pd.Series): cell containing the multilabel target\n",
    "        labels (list): actual list of labels to classify.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Expand of the cell with number of cols\\n\n",
    "        equal to number of element in labels.\n",
    "    \"\"\"\n",
    "    cell_anomalies = [item.strip() for item in cell.split(';')]\n",
    "    splited_cell_anomalies = {label: any(item.startswith(labels) for item in cell_anomalies) for label in labels}\n",
    "    return pd.Series(splited_cell_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c89461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoder(df:pd.DataFrame, target:str, labels:list) -> pd.DataFrame :\n",
    "    \"\"\"Encode the multilabels cells such that each cell is replaced by \\n\n",
    "    a list of same length as labels and containing 0/1.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Task dataframe containing the multilabel target\n",
    "        target (str): The multilabel target in df\n",
    "        labels (list): actual list of labels to classify.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the encoded target\n",
    "    \"\"\"\n",
    "    data = df\n",
    "    encoding_series = data[target].apply(lambda cell: encode_cell(cell, labels))\n",
    "    data[target] = encoding_series.values.tolist()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"PyTorch custom Dataset class. The PyTorch DataLoader will wrap an iterable\\n\n",
    "    around this CustomDataset to enable easy access to the samples.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataframe: pd.DataFrame,\n",
    "                 tokenizer: BertTokenizer,\n",
    "                 max_len: int) -> None:\n",
    "        \"\"\" This function is run once when instantiating\\n\n",
    "        the Dataset object. \n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): Dataset object\n",
    "            tokenizer (BertTokenizer): Tokenizer\n",
    "            max_len (int): Model max lengh\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.narrative = dataframe.Narrative\n",
    "        self.targets = self.data.Anomaly\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of samples in the dataframe.\n",
    "\n",
    "        Returns:\n",
    "            int: number of samples in the dataframe\n",
    "        \"\"\"\n",
    "        return len(self.narrative)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index:int) -> dict:\n",
    "        \"\"\"Loads and returns a sample from the dataframe\\n\n",
    "        at the given index. \n",
    "\n",
    "        Args:\n",
    "            index (int): index\n",
    "\n",
    "        Returns:\n",
    "            dict: Training inputs\n",
    "        \"\"\"\n",
    "        narrative = str(self.narrative.iloc[index])\n",
    "        narrative = \" \".join(narrative.split())\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            narrative,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets.iloc[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c794e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    \"\"\"PyTorch neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" This function is run once when instantiating\\n\n",
    "        the Dataset object.\n",
    "        \"\"\"\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = nn.Dropout(0.3)\n",
    "        self.l3 = nn.Linear(768, 14)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids,\n",
    "                             attention_mask = mask,\n",
    "                             token_type_ids = token_type_ids,\n",
    "                             return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a0fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(model,\n",
    "                     loss_func,\n",
    "                     optimizer,\n",
    "                     epochs,\n",
    "                     dataloader,\n",
    "                     device) :\n",
    "    epoch = 1\n",
    "    while epoch <= epochs :\n",
    "        model.train() # tell PyTorch i'm training the model\n",
    "        size = len(dataloader.dataset)\n",
    "        for batch, data in enumerate(dataloader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(outputs, targets)\n",
    "            if batch % 1000 == 0:\n",
    "                current = (batch + 1) * len(targets)\n",
    "                print(f\"Epoch: {epoch}, loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pt_model(model,\n",
    "                  epochs,\n",
    "                  testingloader,\n",
    "                  device):\n",
    "    epoch = 1\n",
    "    while epoch <= epochs :\n",
    "        model.eval()\n",
    "        fin_targets=[]\n",
    "        fin_outputs=[]\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(testingloader, 0):\n",
    "                ids = data['ids'].to(device, dtype = torch.long)\n",
    "                mask = data['mask'].to(device, dtype = torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "                targets = data['targets'].to(device, dtype = torch.float)\n",
    "                outputs = model(ids, mask, token_type_ids)\n",
    "                fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        fin_outputs = np.array(fin_outputs) >= 0.5\n",
    "        accuracy = metrics.accuracy_score(fin_targets, outputs)\n",
    "        f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "        f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "        print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "safeAeroTokenizer = AutoTokenizer.from_pretrained(\"NASA-AIML/MIKA_SafeAeroBERT\")\n",
    "safeAeroModel = AutoModelForMaskedLM.from_pretrained(\"NASA-AIML/MIKA_SafeAeroBERT\")\n",
    "bertTokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bertModel = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 8\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 8\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73333b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safran-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
